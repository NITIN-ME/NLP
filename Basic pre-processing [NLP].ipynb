{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If', 'you', 'want', 'to', 'achieve', 'greatness', 'stop', 'asking', 'for', 'permission', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "### TOKENIZATION\n",
    "0\n",
    "\n",
    "\"\"\"Download only if needed\"\"\"\n",
    "#nltk.download('punkt')\n",
    "\n",
    "sentence = 'If you want to achieve greatness stop asking for permission.'\n",
    "\n",
    "#print(sentence)\n",
    "\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('If', 'IN'), ('you', 'PRP'), ('want', 'VBP'), ('to', 'TO'), ('achieve', 'VB'), ('greatness', 'NN'), ('stop', 'NN'), ('asking', 'VBG'), ('for', 'IN'), ('permission', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### PoS Tagging\n",
    "\n",
    "\"\"\"Download only if needed\"\"\"\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "word_tag = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "print(word_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you want to achieve greatness stop asking for permission.\n",
      "If want achieve greatness stop asking permission .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Stop Word Removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\"\"\"Download only if needed\"\"\"\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "sentence_stop_word_filered = ' '.join([word for word in tokenized_sentence if word not in stop_words])\n",
    "\n",
    "print(sentence)\n",
    "print(sentence_stop_word_filered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaarbage in garbege out\n",
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n",
      "autocorrect.spell is deprecated, use autocorrect.Speller instead\n",
      "garbage in garbage out\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Spelling correction\n",
    "\n",
    "from autocorrect import spell\n",
    "\n",
    "sentence = 'Gaarbage in garbege out'\n",
    "arr = sentence.split(\" \")\n",
    "\n",
    "print(' '.join(arr))\n",
    "\n",
    "for i in range(len(arr)):\n",
    "    arr[i] = spell(arr[i])\n",
    "\n",
    "print(' '.join(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Stemming\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "print(stemmer.stem(\"coming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\"\"\"Download only if needed\"\"\"\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('cats'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('GPE', [('Tokyo', 'NNP')]), Tree('GPE', [('Japan', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Named Entity Recognition (NER)\n",
    "\n",
    "\"\"\"Download only if needed\"\"\"\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "sentence = 'Tokyo is the capital of Japan.'\n",
    "\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "#print(tokenized_sentence)\n",
    "\n",
    "PoS_tags = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "#print(PoS_tags)\n",
    "\n",
    "named_chunk = nltk.ne_chunk(PoS_tags)\n",
    "\n",
    "#print(named_chunk)\n",
    "\"GPE Stands for Geo-Political Entities\"\n",
    "print([item for item in named_chunk if len(item) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('deposit.v.02')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Word Sense Disambiguation\n",
    "\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "sentence = 'Let us make a bank.'\n",
    "\n",
    "print(lesk(word_tokenize(sentence), 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At the age of 17, in 1989, Musk moved to Canada to attend Queen's University, avoiding mandatory service in the South African military.\n",
      "\n",
      "\n",
      "----------------------------\n",
      "\n",
      "\n",
      "He left in 1992 to study business and physics at the University of Pennsylvania; he graduated with a Bachelor of Science degree in economics and a Bachelor of Arts degree in physics.\n",
      "\n",
      "\n",
      "----------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Sentence boundary detection\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences = \"\"\"\n",
    "At the age of 17, in 1989, Musk moved to Canada to attend Queen's University, avoiding mandatory service in the South African military. He left in 1992 to study business and physics at the University of Pennsylvania; he graduated with a Bachelor of Science degree in economics and a Bachelor of Arts degree in physics.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "list_of_sentences = sent_tokenize(sentences)\n",
    "\n",
    "for item in list_of_sentences:\n",
    "    print(item)\n",
    "    print(\"\\n\\n----------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
